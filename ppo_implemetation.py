# -*- coding: utf-8 -*-
"""PPO Implemetation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a967UTVIPgTeJrkcO_Jf3Yq133mR0Xk0

# Task
Add comments to the provided Python code for a PPO agent, explaining each section, including the environment setup, network definitions, training functions, loss calculations, utility functions, plotting, and the main training loop. Also, address the user's question about using a single optimizer for both the actor and critic networks in PPO.

## Add comments to the code cells for environment setup and library imports.

### Subtask:
Add comments to the code cells that handle setting up the environment and importing necessary libraries.

**Reasoning**:
The first step is to add comments to the first code cell to explain the installation of the required libraries.
"""

# Install necessary libraries for reinforcement learning (gymnasium), deep learning (torch),
# numerical operations (numpy), and plotting (matplotlib).
# !pip install gymnasium torch numpy matplotlib --quiet

"""**Reasoning**:
Add comments to the second and third code cells explaining the library imports and environment creation as per the instructions.


"""

# Import various libraries:
# gymnasium for creating and interacting with RL environments.
# torch.nn for defining neural network modules.
# torch.nn.functional for common neural network operations (like activation functions).
# numpy for numerical operations.
# matplotlib.pyplot for plotting results.
# torch for tensor operations and deep learning.
# torch.distributions for probability distributions used in policy gradients.
# torch.utils.data.TensorDataset and DataLoader for handling data in batches.
# torch.optim.lr_scheduler for adjusting learning rates.
# torch.optim for optimization algorithms.
import gymnasium as gym
import torch.nn as nn
import torch.nn.functional as f
import numpy as np
import numpy
import matplotlib.pyplot as plt
import torch
import torch.distributions as distributions
from torch.utils.data import TensorDataset, DataLoader
import torch.optim as optim

"""**Reasoning**:
Add comments to the third code cell explaining the creation of the training and testing environments as per the instructions.


"""

# Create training and testing environments for the 'CartPole-v1' task.
# gymnasium.make() creates an instance of the specified environment.
env_train = gym.make('CartPole-v1',)
env_test = gym.make('CartPole-v1')

"""## Add comments to the `backbonenetwork` and `actorcritic` class definitions.

### Subtask:
Add comments explaining the purpose and structure of the `BackboneNetwork` and `ActorCritic` class definitions.

**Reasoning**:
Add comments to the BackboneNetwork and ActorCritic class definitions as requested by the subtask.
"""

class BackboneNetwork(nn.Module):
    # This class defines a simple feedforward neural network to be used as a backbone
    # for both the actor and critic networks.
    def __init__(self, in_features, hidden_dimensions, out_features, dropout):
        # Initialize the network layers.
        # layer1: Linear transformation from input features to hidden dimensions.
        super().__init__()
        self.layer1 = nn.Linear(in_features, hidden_dimensions)
        # layer2: Linear transformation between hidden layers.
        self.layer2 = nn.Linear(hidden_dimensions, hidden_dimensions)
        # layer3: Linear transformation from hidden dimensions to the output features.
        self.layer3 = nn.Linear(hidden_dimensions, out_features)
        # dropout: Dropout layer for regularization to prevent overfitting.
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # Define the forward pass of the network.
        # Apply the first linear layer.
        x = self.layer1(x)
        # Apply the ReLU activation function.
        x = f.relu(x)
        # Apply dropout.
        x = self.dropout(x)
        # Apply the second linear layer.
        x = self.layer2(x)
        # Apply the ReLU activation function.
        x = f.relu(x)
        # Apply dropout.
        x = self.dropout(x)
        # Apply the third linear layer to get the final output.
        x = self.layer3(x)
        return x

class ActorCritic(nn.Module):
    # This class combines the actor and critic networks into a single model.
    def __init__(self, actor, critic):
        # Initialize the ActorCritic model with separate actor and critic network instances.
        super().__init__()
        self.actor = actor
        self.critic = critic

    def forward(self, state):
        # Define the forward pass for the ActorCritic model.
        # Get the action predictions from the actor network based on the current state.
        action_pred = self.actor(state)
        # Get the value predictions from the critic network based on the current state.
        value_pred = self.critic(state)
        # Return both the action predictions and the value predictions.
        return action_pred, value_pred

"""## Add comments to the `create agent`, forward pass, and update policy functions.

### Subtask:
Add comments explaining the functionality of the `create_agent` function, the forward pass logic, and the policy update function.

**Reasoning**:
Add comments to the `create_agent`, `forward_pass`, and `update_policy` functions as requested by the subtask.
"""

def create_agent(hidden_dimensions, dropout):
    # This function creates an instance of the ActorCritic agent.
    # It initializes the actor and critic networks using the BackboneNetwork class
    # with specified input features, hidden dimensions, output features, and dropout rate.
    INPUT_FEATURES = env_train.observation_space.shape[0]  # Input features are the dimensions of the observation space.
    HIDDEN_DIMENSIONS = hidden_dimensions  # Number of neurons in the hidden layers.
    ACTOR_OUTPUT_FEATURES = env_train.action_space.n  # Actor output is the number of possible actions.
    CRITIC_OUTPUT_FEATURES = 1  # Critic output is a single value representing the state value.
    DROPOUT = dropout  # Dropout rate for regularization.

    # Instantiate the actor network.
    actor = BackboneNetwork(
            INPUT_FEATURES, HIDDEN_DIMENSIONS, ACTOR_OUTPUT_FEATURES, DROPOUT)
    # Instantiate the critic network.
    critic = BackboneNetwork(
            INPUT_FEATURES, HIDDEN_DIMENSIONS, CRITIC_OUTPUT_FEATURES, DROPOUT)

    # Combine the actor and critic into an ActorCritic model.
    agent = ActorCritic(actor, critic)
    return agent

def init_training():
    # This function initializes lists and variables at the beginning of each training episode.
    # It prepares containers to store episode data and resets the episode reward and 'done' flag.
    states = []  # List to store states visited during the episode.
    actions = []  # List to store actions taken during the episode.
    actions_log_probability = []  # List to store log probabilities of the taken actions.
    values = []  # List to store predicted values of the states.
    rewards = []  # List to store rewards received during the episode.
    done = False  # Flag to indicate if the episode is finished.
    episode_reward = 0  # Accumulator for the total reward in the episode.
    return states, actions, actions_log_probability, values, rewards, done, episode_reward


def forward_pass(env, agent, optimizer, discount_factor):
    # This function simulates one episode of the environment using the current policy.
    # It collects states, actions, action log probabilities, values, and rewards.

    # Initialize lists to store episode data.
    states, actions, actions_log_probability, values, rewards, done, episode_reward = init_training()

    # Reset the environment to start a new episode.
    state = env.reset()[0]

    # Set the agent to training mode.
    agent.train()

    # Loop until the episode is done.
    while not done:
        # Convert the state to a PyTorch tensor and add a batch dimension.
        state = torch.FloatTensor(state).unsqueeze(0)
        # Store the state.
        states.append(state)

        # Perform a forward pass through the agent to get action predictions and value predictions.
        action_pred, value_pred = agent(state)

        # Calculate action probabilities using softmax.
        action_prob = f.softmax(action_pred, dim=-1)
        # Create a categorical distribution over the action probabilities.
        dist = distributions.Categorical(action_prob)

        # Sample an action from the distribution.
        action = dist.sample()
        # Get the log probability of the sampled action.
        log_prob_action = dist.log_prob(action)

        # Take a step in the environment with the sampled action.
        state, reward, terminated, truncated, _ = env.step(action.item())
        done = terminated or truncated
        # Store the action, its log probability, the value prediction, and the reward.
        actions.append(action)
        actions_log_probability.append(log_prob_action)
        values.append(value_pred)
        rewards.append(reward)

        # Accumulate the episode reward.
        episode_reward += reward

    # Concatenate the collected data into tensors.
    states = torch.cat(states)
    actions = torch.cat(actions)
    actions_log_probability = torch.cat(actions_log_probability)
    values = torch.cat(values).squeeze(-1)

    # Calculate the discounted returns for the episode.
    returns = calculate_returns(rewards, discount_factor)
    # Calculate the advantages of the taken actions.
    advantages = calculate_advantages(returns, values)

    # Return the episode reward and the collected data for policy updates.
    return episode_reward, states, actions, actions_log_probability, advantages, returns

def update_policy(
        agent,
        states,
        actions,
        actions_log_probability_old,
        advantages,
        returns,
        optimizer,
        ppo_steps,
        epsilon,
        entropy_coefficient):
    # This function updates the agent's policy and value function using the collected data
    # and the PPO algorithm.

    BATCH_SIZE = 128  # Batch size for training.
    total_policy_loss = 0  # Initialize total policy loss.
    total_value_loss = 0  # Initialize total value loss.

    # Detach old action log probabilities and actions from the computation graph.
    actions_log_probability_old = actions_log_probability_old.detach()
    actions = actions.detach()

    # Create a TensorDataset from the collected data.
    training_results_dataset = TensorDataset(
            states,
            actions,
            actions_log_probability_old,
            advantages,
            returns)

    # Create a DataLoader to iterate through the dataset in batches.
    batch_dataset = DataLoader(
            training_results_dataset,
            batch_size=BATCH_SIZE,
            shuffle=False)  # Shuffle is set to False here as we iterate over the whole trajectory

    # Perform multiple PPO update steps.
    for _ in range(ppo_steps):
        # Iterate through the data in batches.
        for batch_idx, (states, actions, actions_log_probability_old, advantages, returns) in enumerate(batch_dataset):
            # Get new action predictions and value predictions for the current batch of states.
            action_pred, value_pred = agent(states)
            # Remove the last dimension from value predictions.
            value_pred = value_pred.squeeze(-1)

            # Calculate action probabilities using softmax.
            action_prob = f.softmax(action_pred, dim=-1)
            # Create a categorical distribution over the new action probabilities.
            probability_distribution_new = distributions.Categorical(
                    action_prob)
            # Calculate the entropy of the new policy distribution.
            entropy = probability_distribution_new.entropy()

            # Estimate the new log probabilities of the actions taken in the episode
            # using the updated policy.
            actions_log_probability_new = probability_distribution_new.log_prob(actions)

            # Calculate the surrogate loss using the old and new action log probabilities and advantages.
            surrogate_loss = calculate_surrogate_loss(
                    actions_log_probability_old,
                    actions_log_probability_new,
                    epsilon,
                    advantages)

            # Calculate the total policy loss (including entropy bonus) and the value loss.
            policy_loss, value_loss = calculate_losses(
                    surrogate_loss,
                    entropy,
                    entropy_coefficient,
                    returns,
                    value_pred)

            # Zero the gradients, perform backpropagation for both losses, and update the network weights.
            optimizer.zero_grad()
            policy_loss.backward()
            value_loss.backward()
            optimizer.step()

            # Accumulate the losses for reporting.
            total_policy_loss += policy_loss.item()
            total_value_loss += value_loss.item()

    # Return the average policy and value losses over the PPO steps.
    return total_policy_loss / ppo_steps, total_value_loss / ppo_steps

"""## Add comments to the loss calculation and utility functions.

### Subtask:
Add comments explaining the purpose and implementation of the loss calculation functions and any utility functions used.

**Reasoning**:
Add comments to the loss calculation and utility functions as requested by the subtask instructions.
"""

def calculate_returns(rewards, discount_factor):
    # This function calculates the discounted returns for an episode.
    # The return at each time step is the sum of future rewards, discounted by the discount factor.
    returns = []
    cumulative_reward = 0
    # Iterate through rewards in reverse order.
    for r in reversed(rewards):
        # Calculate the cumulative discounted reward.
        cumulative_reward = r + cumulative_reward * discount_factor
        # Insert the cumulative reward at the beginning of the returns list.
        returns.insert(0, cumulative_reward)
    # Convert the list of returns to a PyTorch tensor.
    returns = torch.tensor(returns)
    # Normalize the returns to have zero mean and unit variance.
    returns = (returns - returns.mean()) / returns.std()
    return returns

def calculate_advantages(returns, values):
    # This function calculates the advantages, which represent how much better
    # an action was compared to the expected value of the state.
    # Advantage is calculated as the difference between the actual returns and the predicted values.
    advantages = returns - values
    # Normalize the advantages to have zero mean and unit variance.
    advantages = (advantages - advantages.mean()) / advantages.std()
    return advantages

def calculate_surrogate_loss(
        actions_log_probability_old,
        actions_log_probability_new,
        epsilon,
        advantages):
    # This function calculates the PPO surrogate loss.
    # It uses the ratio of new and old policy probabilities, clipped to prevent large updates.

    # Detach advantages from the computation graph to prevent gradients from flowing through it.
    advantages = advantages.detach()

    # Calculate the ratio of the new policy's probability to the old policy's probability.
    policy_ratio = (
            actions_log_probability_new - actions_log_probability_old
            ).exp()

    # Calculate the first term of the surrogate loss: ratio * advantages.
    surrogate_loss_1 = policy_ratio * advantages

    # Calculate the second term of the surrogate loss: clipped ratio * advantages.
    # The ratio is clipped between 1 - epsilon and 1 + epsilon.
    surrogate_loss_2 = torch.clamp(
            policy_ratio, min=1.0-epsilon, max=1.0+epsilon
            ) * advantages

    # The surrogate loss is the minimum of the two terms (clipped and unclipped).
    surrogate_loss = torch.min(surrogate_loss_1, surrogate_loss_2)
    return surrogate_loss

def calculate_losses(
        surrogate_loss, entropy, entropy_coefficient, returns, value_pred):
    # This function calculates the total policy loss and the value loss.
    # The policy loss is the negative of the surrogate loss plus an entropy bonus.
    # The value loss is the smooth L1 loss between the returns and the predicted values.

    # Calculate the entropy bonus to encourage exploration.
    entropy_bonus = entropy_coefficient * entropy

    # Calculate the policy loss (negative of surrogate loss + entropy bonus).
    # We negate it because optimizers minimize loss, and we want to maximize the objective.
    policy_loss = -(surrogate_loss + entropy_bonus).sum()

    # Calculate the value loss using smooth L1 loss.
    # This measures the difference between the predicted state values and the actual returns.
    value_loss = f.smooth_l1_loss(returns, value_pred).sum()

    return policy_loss, value_loss


def evaluate(env, agent):
    agent.eval()
    rewards = []
    done = False
    episode_reward = 0
    state = env.reset()[0]
    while not done:
        state = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            action_pred, _ = agent(state)
            action_prob = f.softmax(action_pred, dim=-1)
        action = torch.argmax(action_prob, dim=-1)
        state, reward, terminated, truncated, _ = env.step(action.item())
        done = terminated or truncated
        episode_reward += reward
    return episode_reward

"""## Add comments to the plotting functions.

### Subtask:
Add comments explaining the purpose and implementation of the functions used for plotting results.

**Reasoning**:
Add comments to the plotting functions to explain their purpose and how they generate the plots.
"""

def plot_train_rewards(train_rewards, reward_threshold):
    # This function plots the training rewards over episodes.
    # It visualizes how the agent's performance improves during training.
    plt.figure(figsize=(12, 8))  # Set the figure size for better readability.
    plt.plot(train_rewards, label='Training Reward')  # Plot the training rewards.
    plt.xlabel('Episode', fontsize=20)  # Label the x-axis as 'Episode'.
    plt.ylabel('Training Reward', fontsize=20)  # Label the y-axis as 'Training Reward'.
    # Draw a horizontal line indicating the reward threshold for solving the environment.
    plt.hlines(reward_threshold, 0, len(train_rewards), color='y')
    plt.legend(loc='lower right')  # Add a legend to identify the plotted line.
    plt.grid()  # Add a grid for easier reading of values.
    plt.show()  # Display the plot.

def plot_test_rewards(test_rewards, reward_threshold):
    # This function plots the testing rewards over episodes.
    # It shows the agent's performance on a separate test environment,
    # providing an unbiased evaluation of the learned policy.
    plt.figure(figsize=(12, 8))  # Set the figure size.
    plt.plot(test_rewards, label='Testing Reward')  # Plot the testing rewards.
    plt.xlabel('Episode', fontsize=20)  # Label the x-axis as 'Episode'.
    plt.ylabel('Testing Reward', fontsize=20)  # Label the y-axis as 'Testing Reward'.
    # Draw a horizontal line indicating the reward threshold.
    plt.hlines(reward_threshold, 0, len(test_rewards), color='y')
    plt.legend(loc='lower right')  # Add a legend.
    plt.grid()  # Add a grid.
    plt.show()  # Display the plot.

def plot_losses(policy_losses, value_losses):
    # This function plots the policy and value losses over episodes.
    # It helps in monitoring the training process and identifying potential issues
    # like divergence or instability.
    plt.figure(figsize=(12, 8))  # Set the figure size.
    plt.plot(value_losses, label='Value Losses')  # Plot the value losses.
    plt.plot(policy_losses, label='Policy Losses')  # Plot the policy losses.
    plt.xlabel('Episode', fontsize=20)  # Label the x-axis as 'Episode'.
    plt.ylabel('Loss', fontsize=20)  # Label the y-axis as 'Loss'.
    plt.legend(loc='lower right')  # Add a legend to distinguish the loss curves.
    plt.grid()  # Add a grid.
    plt.show()  # Display the plot.

"""## Add comments to the `run ppo` function.

### Subtask:
Add comments explaining the overall process and steps within the `run_ppo` function.

**Reasoning**:
Add comments to the `run_ppo` function to explain the overall training process, hyperparameters, initialization, training loop, function calls within the loop, printing statistics, and plotting results, as requested by the subtask.
"""

def run_ppo():
    # This is the main function that orchestrates the PPO training process.
    # It defines hyperparameters, initializes the agent, runs the training loop,
    # and plots the results.

    # Define hyperparameters for the PPO algorithm and training.
    MAX_EPISODES = 500  # Maximum number of episodes to train for.
    DISCOUNT_FACTOR = 0.99  # Discount factor for calculating returns.
    REWARD_THRESHOLD = 475  # Reward threshold to consider the environment "solved".
    PRINT_INTERVAL = 10  # How often to print training progress (in episodes).
    PPO_STEPS = 8  # Number of PPO update steps to perform per episode.
    N_TRIALS = 100  # Number of recent episodes to average rewards/losses over for printing.
    EPSILON = 0.2  # Clipping parameter for the PPO surrogate loss.
    ENTROPY_COEFFICIENT = 0.01  # Coefficient for the entropy bonus term in the policy loss.
    HIDDEN_DIMENSIONS = 64  # Number of neurons in the hidden layers of the networks.
    DROPOUT = 0.2  # Dropout rate for regularization.
    LEARNING_RATE = 0.001  # Learning rate for the optimizer.

    # Initialize lists to store training and testing metrics across episodes.
    train_rewards = []  # List to store rewards obtained during training episodes.
    test_rewards = []  # List to store rewards obtained during evaluation episodes.
    policy_losses = []  # List to store policy losses.
    value_losses = []  # List to store value losses.

    # Create the ActorCritic agent using the specified hidden dimensions and dropout.
    agent = create_agent(HIDDEN_DIMENSIONS, DROPOUT)
    # Initialize the Adam optimizer with the agent's parameters and learning rate.
    optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE)

    # Start the main training loop, iterating through episodes.
    for episode in range(1, MAX_EPISODES+1):
        # Run one episode in the training environment (forward pass) to collect data.
        # This function returns the episode reward and collected trajectory data
        # (states, actions, old action log probabilities, advantages, returns).
        train_reward, states, actions, actions_log_probability, advantages, returns = forward_pass(
                env_train,
                agent,
                optimizer,
                DISCOUNT_FACTOR)

        # Update the agent's policy and value function using the collected data
        # and the PPO algorithm. This involves multiple optimization steps.
        policy_loss, value_loss = update_policy(
                agent,
                states,
                actions,
                actions_log_probability,
                advantages,
                returns,
                optimizer,
                PPO_STEPS,
                EPSILON,
                ENTROPY_COEFFICIENT)

        # Evaluate the agent's performance on the test environment.
        # This function runs an episode with the current policy and returns the total reward.
        test_reward = evaluate(env_test, agent)

        # Append the calculated losses and rewards to their respective lists.
        policy_losses.append(policy_loss)
        value_losses.append(value_loss)
        train_rewards.append(train_reward)
        test_rewards.append(test_reward)

        # Calculate the mean training and testing rewards and absolute losses
        # over the last N_TRIALS episodes for periodic reporting.
        mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])
        mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])
        mean_abs_policy_loss = np.mean(np.abs(policy_losses[-N_TRIALS:]))
        mean_abs_value_loss = np.mean(np.abs(value_losses[-N_TRIALS:]))

        # Print training progress periodically.
        if episode % PRINT_INTERVAL == 0:
            print(f'Episode: {episode:3} | \
                  Mean Train Rewards: {mean_train_rewards:3.1f} \
                  | Mean Test Rewards: {mean_test_rewards:3.1f} \
                  | Mean Abs Policy Loss: {mean_abs_policy_loss:2.2f} \
                  | Mean Abs Value Loss: {mean_abs_value_loss:2.2f}')

        # Check if the average test reward over the last N_TRIALS episodes meets the threshold.
        # If so, the environment is considered solved, and training stops.
        if mean_test_rewards >= REWARD_THRESHOLD:
            print(f'Reached reward threshold in {episode} episodes')
            break

    # After the training loop finishes (either by reaching max episodes or threshold),
    # plot the training rewards, testing rewards, and losses over time.
    plot_train_rewards(train_rewards, REWARD_THRESHOLD)
    plot_test_rewards(test_rewards, REWARD_THRESHOLD)
    plot_losses(policy_losses, value_losses)

run_ppo()

"""## Summary:

### Q&A

**Can a single optimizer be used for both the actor and critic networks in PPO?**
Yes, a single optimizer can be used for both the actor and critic networks in PPO. As shown in the `run_ppo` function, the `optim.Adam` optimizer is initialized with `agent.parameters()`, which includes parameters from both the actor and critic networks housed within the `ActorCritic` model. During the `update_policy` function, gradients for both the policy loss (derived from the actor) and the value loss (derived from the critic) are computed and used by the *same* optimizer to update the shared parameters or the separate parameters of the actor and critic if they are combined under a single `nn.Module`.

### Data Analysis Key Findings

*   **Environment Setup:** The process involved setting up training and testing environments for the 'CartPole-v1' task using `gymnasium`.
*   **Network Architecture:** A `BackboneNetwork` class was defined as a shared feedforward network structure, and an `ActorCritic` class combined instances of this backbone for the actor (outputting action predictions) and critic (outputting state value predictions).
*   **Agent Creation:** The `create_agent` function instantiates the `ActorCritic` model, setting up the input and output dimensions based on the environment's observation and action spaces.
*   **Data Collection (`forward_pass`):** The `forward_pass` function simulates an episode, collecting states, actions, action log probabilities, value predictions, and rewards. It calculates discounted returns and advantages.
*   **Policy Update (`update_policy`):** The `update_policy` function performs multiple PPO update steps per episode using collected data in batches. It calculates the surrogate policy loss (with clipping and entropy bonus) and the value loss (using smooth L1 loss) and updates the agent's parameters using a single optimizer.
*   **Loss Calculations:** Utility functions calculate discounted returns, advantages (returns minus values), the PPO clipped surrogate loss based on the ratio of new and old policies, and combine the policy and value losses.
*   **Training Initialization:** The `init_training` function initializes lists and variables at the start of each episode to store trajectory data.
*   **Evaluation:** An `evaluate` function (though not explicitly detailed in the comments added, its role is commented in `run_ppo`) assesses the agent's performance on a test environment.
*   **Plotting:** Functions are provided to visualize training rewards, testing rewards, and policy/value losses over episodes, including a horizontal line indicating the reward threshold.
*   **Training Loop (`run_ppo`):** The main training loop iterates for a maximum number of episodes or until a reward threshold is met. It sequentially calls `forward_pass` for data collection, `update_policy` for network updates, and `evaluate` for performance assessment. It also periodically prints training statistics and plots results at the end.
*   **Hyperparameters:** Key hyperparameters for the PPO algorithm (e.g., `MAX_EPISODES`, `DISCOUNT_FACTOR`, `PPO_STEPS`, `EPSILON`, `ENTROPY_COEFFICIENT`, `LEARNING_RATE`) are defined and used within the `run_ppo` function.

### Insights or Next Steps

*   The comprehensive commenting added throughout the code provides a strong foundation for understanding the implementation details of a PPO agent for the CartPole environment.
*   A potential next step could involve experimenting with different hyperparameters or network architectures to further optimize the agent's performance and training speed.

"""

